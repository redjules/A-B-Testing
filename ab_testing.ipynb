{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36615b64-93fa-49ec-b81b-e78e47054372",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5a737b-289c-45d9-b0eb-29d2637f6736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /Users/anapedra/anaconda3/lib/python3.10/site-packages (1.12.1)\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp310-none-macosx_10_9_x86_64.whl (143.4 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.0.2-cp310-cp310-macosx_10_9_x86_64.whl (3.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/anapedra/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "Successfully installed torch-2.0.1 torchaudio-2.0.2 torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "# use this cell to install packages if needed\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ec61e6-e942-4fe0-b22f-209004659510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/anapedra/anaconda3/lib/python3.10/site-packages (4.24.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.24.0\n",
      "    Uninstalling transformers-4.24.0:\n",
      "      Successfully uninstalled transformers-4.24.0\n",
      "Successfully installed huggingface-hub-0.14.1 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "# use this cell to install packages if needed\n",
    "!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a555ef-ba8f-455a-8871-3ed4a3638fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/anapedra/anaconda3/lib/python3.10/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df9b84f-35d7-46c9-9d30-9cd92bdb2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting protobuf>=3.19.6\n",
      "  Downloading protobuf-4.23.2-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.54.2.tar.gz (23.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from tensorboard) (65.6.3)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/anapedra/anaconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: grpcio\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpcio: filename=grpcio-1.54.2-cp310-cp310-macosx_10_10_x86_64.whl size=4090442 sha256=48fc44bf7d46df4abec3f86c505f86973661fdaa2babffb59940019cf64d5971\n",
      "  Stored in directory: /Users/anapedra/Library/Caches/pip/wheels/cb/30/86/ed846d9ef9447372ba80a74992919a1257d827024f3324c535\n",
      "Successfully built grpcio\n",
      "Installing collected packages: tensorboard-data-server, rsa, protobuf, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.1 google-auth-2.19.0 google-auth-oauthlib-1.0.0 grpcio-1.54.2 oauthlib-3.2.2 protobuf-4.23.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9548b9-d8e8-4fc2-8258-0cbae0071b12",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece7e38e-92cb-4b49-8a2f-b62ba3732efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import timeit\n",
    "import collections\n",
    "import time\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, squad_convert_examples_to_features\n",
    "from transformers.data.processors.squad import SquadV2Processor,SquadResult\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a8d207-a875-4b7e-a54a-79ffdd5f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_LOWER_CASE = True\n",
    "NBEST_SIZE = 20\n",
    "DOC_STRIDE = 128\n",
    "MAX_SEQ_LENGTH = 384\n",
    "MAX_QUERY_LENGTH = 64\n",
    "MAX_ANSWER_LENGTH = 30\n",
    "DATA_DIR = 'data/squad'\n",
    "PREDICT_FILE = 'dev-v2.0.json'\n",
    "\n",
    "BERT_MODEL_TYPE = 'bert'\n",
    "BERT_MODEL_HF_PATH = \"twmkn9/bert-base-uncased-squad2\"\n",
    "BERT_OUTPUT_DIR = \"models/bert/twmkn9_bert-base-uncased-squad2\"\n",
    "\n",
    "DISTILBERT_MODEL_TYPE = 'distilbert'\n",
    "DISTILBERT_MODEL_HF_PATH = 'twmkn9/distilbert-base-uncased-squad2'\n",
    "DISTILBERT_OUTPUT_DIR = 'models/distilbert/twmkn9_distilbert-base-uncased-squad2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa96291-d796-4932-91d8-32dba020f40e",
   "metadata": {},
   "source": [
    "# Q&A Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95794391-e187-4b6d-9454-c1141ee536da",
   "metadata": {},
   "source": [
    "Measuring the success of Q&A systems is complicated. When asked a question like \"Why the sky is Blue?\", there are several potential right answers. For instance, one could refer to \"Rayleigh Scattering\" or another answer could be:\n",
    "```\n",
    "The Earth's atmosphere scatters short-wavelength light more efficiently than that of longer wavelengths. Because its wavelengths are shorter, blue light is more strongly scattered than the longer-wavelength lights, red or green. Hence the result that when looking at the sky away from the direct incident sunlight, the human eye perceives the sky to be blue.\n",
    "```\n",
    "\n",
    "Both options are correct and referred in Wikipedia Artile 'Diffuse Sky Radiation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e1e4a-0f04-4243-b047-b5906b811b5e",
   "metadata": {},
   "source": [
    "Most Q&A systems rely on a corpus of information that is initially indexed by an information retrieval system. Then, snippets of text are extracted where the Q&A model scores the most likely sentences to answer a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038d049-742e-4a06-8918-27e80bb4065e",
   "metadata": {},
   "source": [
    "However, the same snippet of text that answers the blue sky question, may not be able to answer a similar query like \"Could the Sky ever be green?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ae2a0-8808-45af-ab09-c4835f9d87bb",
   "metadata": {},
   "source": [
    "This is the gray area for measuring performance of Q&A models. How should we judge a model's success when there are multiple correct answers, even more incorrect answers, and, potentially no answers available to it at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ec92d-0a92-438b-a59a-3377fa9fe815",
   "metadata": {},
   "source": [
    "# SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d3043-6925-4d1e-8196-d05fc50bf59a",
   "metadata": {},
   "source": [
    "[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc526c9-3b2b-433d-b20f-1a47acdaec30",
   "metadata": {},
   "source": [
    "```\n",
    "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f372f-a6a7-4a41-adb8-23c9fdb558da",
   "metadata": {},
   "source": [
    "```\n",
    "SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3125d-dc6b-4088-98fe-cdf4927f5e3c",
   "metadata": {},
   "source": [
    "We target the SQuAD 2.0 as it represents a scenario that is closer to the real world: **It includes additional questions that cannot be answered by the accompanying passage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2fadfb-ef8f-4d5c-90dd-86dcdc63f148",
   "metadata": {},
   "source": [
    "**Download the Squad dev set for model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1a0279-9ce3-4092-a704-6a25fe641440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b6f47-5f71-4125-b674-d8be7087b98d",
   "metadata": {},
   "source": [
    "We will make extensive use of [Hugging Face](https://huggingface.co/) and [Pytorch](https://pytorch.org/) throughout this code, they provide several implementations for loading datasets and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001de59-39f3-4c92-9366-756cfad6daee",
   "metadata": {},
   "source": [
    "## Loading the DEV set using Hugging Face data processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9398455-4358-409b-a2be-94631719e0fe",
   "metadata": {},
   "source": [
    "We will make use of [Processors](https://huggingface.co/transformers/main_classes/processors.html) to facilitate basic processing tasks with some canonical NLP datasets. The processors can be used for loading datasets and converting their examples to features for direct use in the model. More specifically, we will be using the [SQuAD processors](https://huggingface.co/transformers/main_classes/processors.html#squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a8f60c-ef09-41b3-b40e-e159441db73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e997fe-1d53-4f15-b4a6-f57b76f2272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(model_name_or_path, \n",
    "                            data_dir= DATA_DIR, \n",
    "                            predict_file=PREDICT_FILE, \n",
    "                            max_seq_length=MAX_SEQ_LENGTH, \n",
    "                            doc_stride=DOC_STRIDE, \n",
    "                            max_query_length=MAX_QUERY_LENGTH, \n",
    "                            overwrite_cache=True):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "    # Load data features from cache or dataset file\n",
    "    input_dir = data_dir if data_dir else \".\"\n",
    "    cached_features_file = os.path.join(\n",
    "        input_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"dev\",\n",
    "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(max_seq_length),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Init features and dataset from cache if it exists\n",
    "    if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        processor = SquadV2Processor()\n",
    "\n",
    "        examples = processor.get_dev_examples(data_dir, filename=predict_file)\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=1,\n",
    "        )\n",
    "\n",
    "\n",
    "    return dataset, examples, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe46a92-9e36-4fa5-af88-66d18472854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce1fb71f504a83b391b6454bda77c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8395d8b22044b51b47b04770bb5f09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38b37c62f70416db877641a9de9d6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b62f21125947d9882b5a4a0791d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/squad/dev-v2.0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset, examples, features \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBERT_MODEL_HF_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36mload_and_cache_examples\u001b[0;34m(model_name_or_path, data_dir, predict_file, max_seq_length, doc_stride, max_query_length, overwrite_cache)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     processor \u001b[38;5;241m=\u001b[39m SquadV2Processor()\n\u001b[0;32m---> 34\u001b[0m     examples \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dev_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     features, dataset \u001b[38;5;241m=\u001b[39m squad_convert_examples_to_features(\n\u001b[1;32m     37\u001b[0m         examples\u001b[38;5;241m=\u001b[39mexamples,\n\u001b[1;32m     38\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset, examples, features\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/data/processors/squad.py:644\u001b[0m, in \u001b[0;36mSquadProcessor.get_dev_examples\u001b[0;34m(self, data_dir, filename)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdev_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSquadProcessor should be instantiated via SquadV1Processor or SquadV2Processor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdev_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    646\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m    647\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(reader)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_examples(input_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/squad/dev-v2.0.json'"
     ]
    }
   ],
   "source": [
    "dataset, examples, features = load_and_cache_examples(BERT_MODEL_HF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61524b-f118-47cd-8351-5083a5994f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(examples)} in the dev dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e7de1-67fe-4ff4-be14-0524d4b7607c",
   "metadata": {},
   "source": [
    "The list of examples contains objects of type **transformers.data.processors.squad.SquadExample**. We use the function below to extract the information we want from such objects. More specifically: **'qid'**, **'question_text'**, **'context_text'** and **'answer'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4d02f-b329-401c-ae60-15f6a81a37e0",
   "metadata": {},
   "source": [
    "We will firstly create some extra variables to help on manipulation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e01c96-4588-4a66-9814-6bbb3f6ad787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some maps to help us identify examples of interest\n",
    "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
    "qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
    "answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\n",
    "no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12255aad-1bea-48bb-afd9-dfeb5e935c01",
   "metadata": {},
   "source": [
    "And also, the function below to help on extracting information given a `qid` (question unique identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f8fbd-3032-42ca-b358-12cc0c502553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_example(qid:str):    \n",
    "    from pprint import pprint\n",
    "\n",
    "    idx = qid_to_example_index[qid]\n",
    "    q = examples[idx].question_text\n",
    "    c = examples[idx].context_text\n",
    "    a = [answer['text'] for answer in examples[idx].answers]\n",
    "    \n",
    "    print(f'Example {idx} of {len(examples)}\\n---------------------')\n",
    "    print(f\"Q: {q}\\n\")\n",
    "    print(\"Context:\")\n",
    "    pprint(c)\n",
    "    print(f\"\\nTrue Answers:\\n{a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0ee76-9a3e-414a-8833-1b0ec4e403ed",
   "metadata": {},
   "source": [
    "### Positive Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38536778-778a-44e2-b912-c67a2ce9a17d",
   "metadata": {},
   "source": [
    "50% of the examples in the test set are questions that have answers contained within their corresponding passage. In these cases, up to five possible correct answers are provided. Such answers must come directly from the passage, we will see later, however, that there are several ways to arrive at a \"correct\" answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ea24d-d586-40c6-a7d3-d368f519d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example(answer_qids[1300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747683a8-45e6-4b2e-a34b-edfc76fa35da",
   "metadata": {},
   "source": [
    "### Negative Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c52c4-ae4c-455b-94a9-088d967c8a98",
   "metadata": {},
   "source": [
    "The other 50% of questions in the test set do not have an answer. This is important as in a real life Q&A system, our model needs to learn when **to not answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639618f-95d6-4fa0-b32f-64e9f58c3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example(no_answer_qids[1254])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dea49-53e4-4d03-b7ef-5c8e1a2fe069",
   "metadata": {},
   "source": [
    "# Metrics for Q&A Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743867a8-8400-40e5-9b1e-46b936309daa",
   "metadata": {},
   "source": [
    "When measuring the performance of a machine learning system, we need to think about both **model** and **customer** metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4e4bf-6c51-4ad4-9d0c-4f2e03ae7ae6",
   "metadata": {},
   "source": [
    "Q&A systems are usually measured by two dominant metrics: **F1** and **Exact Match (EM)**. They are computed on individual **Question & Answer** pairs. When multiple correct answers are available for a given question, the maximum score over all possible correct answers is computed. Overall **EM** and **F1** scores are computed for a model by averaging over the individual example scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244ebf2-d127-46b1-a7c8-ae1b7cce8908",
   "metadata": {},
   "source": [
    "## Exact Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9672a95-af03-4078-9a81-178bfca91e39",
   "metadata": {},
   "source": [
    "For each Q&A Pair, if the **characters** of the model's prediction are an exact match of the characters of any of the True Answer(s), **EM=1**, otherwise **EM=0**. This is a strict all-or-nothing metric, which may have little value for final customers of a **Q&A System**. It may be beneficial only when assessing against a negative example; if the model predicts any text at all, it automatically receives a **0** for that example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633a18f-8b0c-47fd-a292-c72c153589e2",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3eba87-aa64-4090-9a6f-0c93aa46f923",
   "metadata": {},
   "source": [
    "Almost all classificattion problems rely on F1 score to measure model performance. It is mostly appropriate when we care equally about precision and recall. On a **Q&A system**, however, it is computed over the individual words in the prediction against those in the **True Answer**. The number of shared words between the prediction and the trust is the basis of the F1 score. While **precision** is the ration between the number of shared words to the total number of words in the prediction; Recall is the ratio of the number of shared words to the total number of words in the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f16e3-37a9-4922-a58f-c94e3fb9d0f7",
   "metadata": {},
   "source": [
    "<img src=\"f1score.png\" alt=\"f1score\" width=\"600\" style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57190fef-29a3-440a-bd76-a7a116058e2b",
   "metadata": {},
   "source": [
    "## Latency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da4291-a7ae-4416-b13c-8df21d239180",
   "metadata": {},
   "source": [
    "Lateny is an important metric for ML Systems. In the Q&A example its of the utmost importance when the system is used in a conversational application. For instance: Alexa and Google home are devices that have very strict latency constraints as the uses expects an answer with a few seconds after the question was asked. When updating models we should take this dimension according to the application of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c0ee6-5a11-44c5-96b7-de0d1bad2cdc",
   "metadata": {},
   "source": [
    "## Answer Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ca20f-3665-48b1-8766-2b5581b90b3d",
   "metadata": {},
   "source": [
    "In Q&A Systems, models that attempt to answer every question are often perceived as innacurate. The system should only provide output when confident enough to do so, in other words, when the probabilities of prodictions are above certain threshold. In some applications, a model should also be able to say \"I don't know\" or \"The context has not enough information to answer the question\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ba07b-d0c8-4afd-b87a-d59ae0a6eed4",
   "metadata": {},
   "source": [
    "# Q&A Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55895106-c6e5-48ce-95cc-9e8afcf0768b",
   "metadata": {},
   "source": [
    "Question and Answering makes use of Large Language Models (LLMs) as any other classification problem in NLP. The main difference relies on how the input and output is provided to the model. Generally speaking models are trained to match the **true answer** to the **question** as they are provided together as an input to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fa165-4d44-40a2-9371-60682f8f6fb9",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5995-fe77-4f1b-9ec2-6509430cf5bb",
   "metadata": {},
   "source": [
    "BERT, or Bidirectional Encoder Representations from Transformers, is a neural approach to pre-train language representations which obtains near state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks, including SQuAD Question Answering dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa350a5a-982b-487b-b5e8-09bd3436f446",
   "metadata": {},
   "source": [
    "Developed in 2019 BERT achieves **80.422** in the **EM** score and **83.118** in the **F1** score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a64338-b05e-4ca2-8c85-15733f5eeed8",
   "metadata": {},
   "source": [
    "BERT-base has **110 million** parameters and BERT-large has **340 million** parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b237e1-d1e2-4f2e-8e0c-e814e0405a6d",
   "metadata": {},
   "source": [
    "<img src=\"model_params.png\" alt=\"Model Parameters Comparison\" width=\"1000\" style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc29d31-7d49-4465-a106-9bd6e6a1888e",
   "metadata": {},
   "source": [
    "As Large Language Models were developed, the amount of parameters in these models have grown exponentially. Although this improves model performance it comes at a cost: **Latency**. As we will discuss, for use cases where inference is done on batches that may have less impact, however, on real time systems such as voice assistants or web search, latency plays a major role on deciding whether one model is better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfb572-b559-4f52-8fe3-8a4fe44c85be",
   "metadata": {},
   "source": [
    "### BERT Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3152539-4a94-44ed-8f17-bc4945e8e613",
   "metadata": {},
   "source": [
    "[CLS] context [SEP] question [SEP] [PAD] [PAD] [PAD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564917-160a-4159-845e-77717ada8c0f",
   "metadata": {},
   "source": [
    "**context** = \"The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations.\"\n",
    "\n",
    "**question** = \"What organization is the IPCC a part of?\"\n",
    "\n",
    "**after being merged by the tokenizer**:\n",
    "```\n",
    "\"[CLS] The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations. [SEP] What organization is the IPCC a part of? [SEP] [PAD] [PAD] [PAD]\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d528f-12ab-438e-9e0d-272329ef10a3",
   "metadata": {},
   "source": [
    "**token-id format**:\n",
    "\n",
    "[101, 1109, 11300, 2758, 24472, 15595, 20339, 1113, 13540, 9091, 113, 14274, 12096, 114, 1110, 170, 3812,\n",
    " 9455, 2758, 24472, 15595, 1404, 1223, 1103, 22105, 1104, 1103, 1244, 3854, 119, 102, 1327, 2369, 1110, 1103,\n",
    " 14274, 12096, 170, 1226, 1104, 136, 102, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da16f222-cd54-4e00-a143-41831f5acce2",
   "metadata": {},
   "source": [
    "## Loading Pre Trained BERT from Huggingface repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53babf9-d53d-441e-8764-1d379d0ce058",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_HF_PATH, use_fast=False)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(BERT_MODEL_HF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c7446-0d76-4f43-a147-ef675fbef71e",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec454b17-e778-4266-9672-bed448e27190",
   "metadata": {},
   "source": [
    "Given a **Question ID**, **Model** and **Tokenizer** we get an answer text. In here we get the maximum probability of beginning and end for the answer in the Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97fce6a-dea3-4dbd-a8cd-050c49655615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(qid: str, model:AutoModelForQuestionAnswering, tokenizer:AutoTokenizer):\n",
    "    # given a question id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
    "    question = examples[qid_to_example_index[qid]].question_text\n",
    "    context = examples[qid_to_example_index[qid]].context_text\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(outputs[1]) + 1 \n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d51850-c937-4a0f-9ee9-0fa4f516ca53",
   "metadata": {},
   "source": [
    "We create a simple function that given an **example** list it extracts the gold answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57702ebc-2cb1-415c-911c-9b6fd144e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "    \n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example - \n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "        \n",
    "    return gold_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a5eb1-c87f-40e6-9f8d-ed388ebd4df6",
   "metadata": {},
   "source": [
    "For Metrics like **Exact Match** we need to make sure that texts are normalized so we can compare on a character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d6e48-9e4b-4f69-b118-7be3fbae82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are heavily influenced by the HF squad_metrics.py script\n",
    "def normalize_text(s: str):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d49b6-31d6-4c25-9117-bcdd4685baa5",
   "metadata": {},
   "source": [
    "## Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457dab6-8a72-4fd6-bc80-50eec03582c1",
   "metadata": {},
   "source": [
    "### Exact Match (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103af324-a06e-4d37-9ccc-e6f4013b3b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acaa641-ee7a-4d1b-9ff3-a2ab970148ae",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ae633-ca59-4e71-9141-a0c4a1ab6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3deeb-48a0-49f4-9cc3-bd78f3bb1c78",
   "metadata": {},
   "source": [
    "Computing EM and F1 for an example with a gold answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56521116-e070-498f-bd0c-6855b2a80e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = get_prediction(answer_qids[1303], model, tokenizer, )\n",
    "example = examples[qid_to_example_index[answer_qids[1303]]]\n",
    "\n",
    "gold_answers = get_gold_answers(example)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"Question: {example.question_text}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"True Answers: {gold_answers}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d335931-6a72-43b3-a3aa-0bb3dcd9fd58",
   "metadata": {},
   "source": [
    "Now lets try and compute an example without answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca91a8-f074-4cb4-9146-205b09921772",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = get_prediction(no_answer_qids[1254], model, tokenizer)\n",
    "example = examples[qid_to_example_index[no_answer_qids[1254]]]\n",
    "\n",
    "gold_answers = get_gold_answers(example)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"Question: {example.question_text}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"True Answers: {gold_answers}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82507ee-f9b4-4ac6-bd62-d0acae55abd3",
   "metadata": {},
   "source": [
    "Both metrics are zero, this model does not correctly asses that this question is unanswearable. It predicts the [CLS] token (it means it considers the entire context as an answer to the question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9961e-b5f8-4c9c-8845-678256ecc9e8",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb347d05-7cd3-47f0-8f99-467ceb5f1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_metrics(model:AutoModelForQuestionAnswering,tokenizer: AutoTokenizer, answer_qids=answer_qids, examples=examples):\n",
    "    answers_arr = []\n",
    "    start_time = time.time()\n",
    "    errors = []\n",
    "    for qid in tqdm(answer_qids):\n",
    "        try:\n",
    "            prediction = get_prediction(qid, model, tokenizer)\n",
    "            example = examples[qid_to_example_index[qid]]\n",
    "\n",
    "            gold_answers = get_gold_answers(example)\n",
    "\n",
    "            em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "            f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "\n",
    "            result_dict = {}\n",
    "            result_dict[\"qid\"] = qid\n",
    "            result_dict[\"question\"] = example.question_text\n",
    "            result_dict[\"prediction\"] = prediction\n",
    "            result_dict[\"true_answers\"] = ';'.join(gold_answers)\n",
    "            result_dict[\"f1\"] = f1_score\n",
    "            result_dict[\"em\"] = em_score\n",
    "            answers_arr.append(result_dict)\n",
    "        except:\n",
    "            errors.append(qid)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return pd.DataFrame(answers_arr), end_time-start_time, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ba86b-92b9-49a4-91f4-3d235a646b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df, total_time, errors = get_answers_metrics(model, tokenizer, answer_qids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0d3a4-3556-4f4b-a2ea-cc1d66f05e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470edc2a-08a8-47a5-86c8-00471b6d7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['em'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd33c51-435c-4c7f-befd-de91b434596a",
   "metadata": {},
   "source": [
    "## Improving meausurement functions through model thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b100e-12fa-4928-8a60-f9a50a0079c2",
   "metadata": {},
   "source": [
    "When we tokenize a question and context, and we pass it to the model, the output consists of two probabilities (logits). One is the start of the answer span, the other for the end of the answer span."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1eb3d-5fea-4af4-9e8e-0a4f8dff74cb",
   "metadata": {},
   "source": [
    "Every token that is passed to the model is assigned a logit, and tokens corresponding to the question itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a684e-be92-4799-8c7d-82f7419fa9dd",
   "metadata": {},
   "source": [
    "Lets have a look at what this means, using a previous question (\"What happened 3.7-2 billion years ago?\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a81f2-593d-4c66-9505-619e140cc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(example.question_text, example.context_text, return_tensors='pt')\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdd7d9-e7e9-4e6d-8239-b69437581df2",
   "metadata": {},
   "source": [
    "Looking below, we can observe how large is the first position of the array, this is the [CLS] token position. This has a strong probability that this question has no answer, but we answered it anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cb940-5d43-4096-8155-eeaea792dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output.start_logits\n",
    "end_logits = output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfbd369-c278-45da-9c8b-d837865dcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79508093-0f1f-4ae0-b5f6-fdc3e1f5c185",
   "metadata": {},
   "source": [
    "Our model gets predictions by selecting the start and end tokens with the largest logits. It would be more sensible to choose any sensible start+end combination as possible to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656637c-d26c-4aa5-998c-fe5a6458f9b2",
   "metadata": {},
   "source": [
    "These combinations can be score independently and the one with the highest score would be considered the best answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00e3e3-dc66-4c87-b4fc-ddeab42f28df",
   "metadata": {},
   "source": [
    "A possible (candidate) answer is scored as the sum of its start and end logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47dfb2-289a-4259-bebc-f81497056f0a",
   "metadata": {},
   "source": [
    "### Calculating possible combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76d258-795f-4451-a104-f01e0689d311",
   "metadata": {},
   "source": [
    "We start by taking the n largest start and end logits. Any sensible combination can be considered an answer, however, some consistency checks must first be performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596305b-b34c-4972-a930-333040a7edd5",
   "metadata": {},
   "source": [
    "For instance:\n",
    "    \n",
    "    - End token must fall after the start token\n",
    "    - Candidate answers wherein the start or end tokens are associated with question tokens\n",
    "\n",
    "[CLS] is not removed from the answers as it can indicate null answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed498817-6791-4ee4-a0bd-0a78d2b57f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our start and end logit tensors to lists\n",
    "start_logits = to_list(start_logits)[0]\n",
    "end_logits = to_list(end_logits)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838002c-ee66-4c7d-be75-34d4cb3bc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort our start and end logits from largest to smallest, keeping track of the index\n",
    "start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ec0c7-3e8c-48d9-a87d-85b69b52200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top n (in this case, 5)\n",
    "print(start_idx_and_logit[:5])\n",
    "print(end_idx_and_logit[:5]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d41ed-900e-47dd-8d7e-b2e8b33908a9",
   "metadata": {},
   "source": [
    "The null answer token (index 0) is in the top five of both the start and end logit lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133de477-16b8-4e5c-8167-c9021679095d",
   "metadata": {},
   "source": [
    "In order to eventually predict a text answer (or empty string), we need to keep track of the indexes which will be used to pull the corresponding token ids later on. We'll also need to identify which indexes correspond to the question tokens, so we can ensure we don't allow a nonsensical prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7b1f7-e605-4c85-b129-6e52fc7692fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_indexes = [idx for idx, logit in start_idx_and_logit[:5]]\n",
    "end_indexes = [idx for idx, logit in end_idx_and_logit[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c08bc-9a23-4443-b12e-39a3106eaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the token ids from a tensor to a list\n",
    "tokens = to_list(inputs['input_ids'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7713eb-e505-4ddc-98b1-2203167c5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token \n",
    "question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "question_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1d28a-70fe-4255-bcfc-fddfe7d5f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of all preliminary predictions\n",
    "PrelimPrediction = collections.namedtuple( \n",
    "    \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2682871-69d7-4129-b8f9-433c2b42697e",
   "metadata": {},
   "source": [
    "We'll generate a list of candidate predictions by looping through all combinations of the start and end token indexes, excluding nonsensical combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63a56-8d53-404b-aadb-ed8a33c73eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prelim_preds = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # throw out invalid predictions\n",
    "        if start_index in question_indexes:\n",
    "            continue\n",
    "        if end_index in question_indexes:\n",
    "            continue\n",
    "        if end_index < start_index:\n",
    "            continue\n",
    "        prelim_preds.append(\n",
    "            PrelimPrediction(\n",
    "                start_index = start_index,\n",
    "                end_index = end_index,\n",
    "                start_logit = start_logits[start_index],\n",
    "                end_logit = end_logits[end_index]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015b951-6b2a-4f81-9509-fb85f4df8279",
   "metadata": {},
   "source": [
    "With a list of sensible candidate predictions, it's time to score them.\n",
    "\n",
    "For a candidate answer, score = start_logit + end_logit. Below, we sort our candidate predictions by their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ab9e7-68af-489f-a4a6-2a3a63b8821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort preliminary predictions by their score\n",
    "prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "pprint(prelim_preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a5b63-fe74-4d6c-b2d3-6b247fac502a",
   "metadata": {},
   "source": [
    "We need to convert our preliminary predictions into actual text (or the empty string, if null). We'll keep track of text predictions we've seen, because different token combinations can result in the same text prediction and we only want to keep the one with the highest score (we're looping in descending score order). Finally, we'll trim this list down to the best 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bd7c6-a832-4a3e-836a-dd96b5d011df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of all best predictions\n",
    "BestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "    \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291ef7f-b273-4f85-9c2b-fbeb5d69db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = []\n",
    "seen_predictions = []\n",
    "for pred in prelim_preds:\n",
    "    \n",
    "    # for now we only care about the top 5 best predictions\n",
    "    if len(nbest) >= 5: \n",
    "        break\n",
    "        \n",
    "    # loop through predictions according to their start index\n",
    "    if pred.start_index > 0: # non-null answers have start_index > 0\n",
    "\n",
    "        text = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(\n",
    "                tokens[pred.start_index:pred.end_index+1]\n",
    "            )\n",
    "        )\n",
    "        # clean whitespace\n",
    "        text = text.strip()\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        if text in seen_predictions:\n",
    "            continue\n",
    "\n",
    "        # flag this text as being seen -- if we see it again, don't add it to the nbest list\n",
    "        seen_predictions.append(text) \n",
    "\n",
    "        # add this text prediction to a pruned list of the top 5 best predictions\n",
    "        nbest.append(BestPrediction(text=text, start_logit=pred.start_logit, end_logit=pred.end_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b3a8b-f47d-4bb3-b6b4-01bc13703e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7a1b8-859c-4052-aef3-9a8ecb2d8fdf",
   "metadata": {},
   "source": [
    "At this point, we have a neat list of the top 5 best predictions for this question, lets now also add the null answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9619c51-035e-470a-a204-29ee2be468b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and don't forget -- include the null answer!\n",
    "nbest.append(BestPrediction(text=\"\", start_logit=start_logits[0], end_logit=end_logits[0]))\n",
    "nbest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252e4ee-7102-4ce3-afb7-b72937df3663",
   "metadata": {},
   "source": [
    "The null answer is scored as the sum of the start_logit and end_logit associated with the [CLS] token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f628573-6c90-473b-8bd1-eebf4055c486",
   "metadata": {},
   "source": [
    "The last step is to compute the null score -- more specifically, the difference between the null score and the best non-null score as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a19a4-96c2-4e60-87dc-0fc319eaa00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the null score as the sum of the [CLS] token logits\n",
    "score_null = start_logits[0] + end_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4feba9-7914-46ec-b8d2-c25c47fa90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d40f13-903f-4622-880d-08fefeee293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest[0].start_logit + nbest[0].end_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0941d66d-5191-4b0f-80bb-d91c310f0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between the null score and the best non-null score\n",
    "score_diff = score_null - nbest[0].start_logit - nbest[0].end_logit\n",
    "\n",
    "score_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958be2d-5e06-4df8-a74c-7bb507c518db",
   "metadata": {},
   "source": [
    "## SQuAD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f8677-39b9-4e92-9ce2-07cf76d126af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name_or_path, \n",
    "             dataset, \n",
    "             output_dir, \n",
    "             per_gpu_eval_batch_size=12, \n",
    "             n_gpu=1, \n",
    "             model_type=BERT_MODEL_TYPE,\n",
    "             do_lower_case=DO_LOWER_CASE,\n",
    "             nbest_size=NBEST_SIZE,\n",
    "             max_answer_length=MAX_ANSWER_LENGTH,\n",
    "             null_score_diff_threshold=0.0):\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    eval_batch_size = per_gpu_eval_batch_size * max(1, n_gpu)\n",
    "\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "    all_results = []\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "\n",
    "            start_logits, end_logits = output\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    print(f\"Evaluation done in total {evalTime} secs ({evalTime/len(dataset)} sec per example)\")\n",
    "\n",
    "    # Compute predictions\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
    "\n",
    "\n",
    "    output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
    "\n",
    "\n",
    "    predictions = compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        nbest_size,\n",
    "        max_answer_length,\n",
    "        do_lower_case,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        False,\n",
    "        True,\n",
    "        null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    # Compute the F1 and exact scores.\n",
    "    results = squad_evaluate(examples, predictions)\n",
    "    \n",
    "    results.update({\"eval_time\": evalTime, 'prediction_time': evalTime/len(dataset)})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434bee6-7640-4c7f-8a2b-32eeda8cd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(BERT_MODEL_HF_PATH, dataset, BERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47497a82-6e59-4693-9118-a957d0cc297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4d7fc-fea2-4e1b-95da-afdbdff70720",
   "metadata": {},
   "source": [
    "The first three blocks of the Results output are pretty straightforward. EM and F1 scores are reported over a) the full dev set, b) the set of positive examples, and c) the set of negative examples. This can provide some insight into whether a model is performing adequately on both answer and no-answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014ea6b-8844-4676-a3eb-e36a28abf627",
   "metadata": {},
   "source": [
    "As per below results, we see that two sections are zero. They depend on setting a threshold for the model so it knows when to prefer a null answer over an actual answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8ac6b-297f-477e-b84e-a0909f86cd10",
   "metadata": {},
   "source": [
    "In other words, we should predict a null answer for a given example if that example's score difference is above a certain threshold. What should that threshold be? How should we compute it? They give us a recipe: select the threshold that maximizes F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b2c5c-f098-4dea-b1f7-31603cc1f57b",
   "metadata": {},
   "source": [
    "We will leverage the files created by the evaluate function above to retrieve the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52960f98-54c1-435d-8cf2-628797be9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predictions we generated earlier\n",
    "filename = BERT_OUTPUT_DIR + '/predictions.json'\n",
    "preds = json.load(open(filename, 'rb'))\n",
    "\n",
    "# load the null score differences we generated earlier\n",
    "filename = BERT_OUTPUT_DIR + '/null_odds.json'\n",
    "null_odds = json.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f7c13-8d59-4508-b734-948af84e2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default threshold is set to 1.0 -- we'll leave it there for now\n",
    "results_default_thresh = squad_evaluate(examples, \n",
    "                                        preds, \n",
    "                                        no_answer_probs=null_odds, \n",
    "                                        no_answer_probability_threshold=1.0)\n",
    "\n",
    "pprint(results_default_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd60467-debf-4e80-8594-e70a6d82dacf",
   "metadata": {},
   "source": [
    "The first three blocks have identical values as in our initial evaluation because they are based on the default threshold (which is currently 1.0). However, the values in the fourth block have been updated by taking into account the null_odds information. When a given example's score_diff is greater than the threshold, the prediction is flipped to a null answer which affects the overall EM and F1 scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e261d-e749-4482-8925-629d1cd7866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_thresh = -3.415079116821289\n",
    "results_f1_thresh = squad_evaluate(examples, \n",
    "                                   preds, \n",
    "                                   no_answer_probs=null_odds, \n",
    "                                   no_answer_probability_threshold=best_f1_thresh)\n",
    "\n",
    "pprint(results_f1_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48420f7d-e519-4036-972a-d84a55c52ba5",
   "metadata": {},
   "source": [
    "We can see that metrics for NoAns have increased significantly. The downside is that we lose some ground in how well our model correctly predicts HasAns examples. Overall, however, we see a net increase of a couple points in both EM and F1 scores. This demonstrates that computing null scores and properly using a null threshold significantly increases QA performance on the SQuAD2.0 dev set with almost no additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498a959-eb5a-44c1-9a7e-e98e48b9f78e",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e788021-6ddd-4e81-8251-30b4c1ddf327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_inputs(example, tokenizer):\n",
    "    # load the example, convert to inputs, get model outputs\n",
    "    question = example.question_text\n",
    "    context = example.context_text\n",
    "    return tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "def get_clean_text(tokens, tokenizer):\n",
    "    text = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(tokens)\n",
    "        )\n",
    "    # Clean whitespace\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def prediction_probabilities(predictions):\n",
    "\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    all_scores = [pred.start_logit+pred.end_logit for pred in predictions] \n",
    "    return softmax(np.array(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29420b0-7b57-48b0-bba9-405b8f72324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminary_predictions(start_logits, end_logits, input_ids, nbest):\n",
    "    # convert tensors to lists\n",
    "    start_logits = to_list(start_logits)[0]\n",
    "    end_logits = to_list(end_logits)[0]\n",
    "    tokens = to_list(input_ids)[0]\n",
    "\n",
    "    # sort our start and end logits from largest to smallest, keeping track of the index\n",
    "    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]]\n",
    "    end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]]\n",
    "\n",
    "    # question tokens are between the CLS token (101, at position 0) and first SEP (102) token \n",
    "    question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "\n",
    "    # keep track of all preliminary predictions\n",
    "    PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "    prelim_preds = []\n",
    "    for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "            # throw out invalid predictions\n",
    "            if start_index in question_indexes:\n",
    "                continue\n",
    "            if end_index in question_indexes:\n",
    "                continue\n",
    "            if end_index < start_index:\n",
    "                continue\n",
    "            prelim_preds.append(\n",
    "                PrelimPrediction(\n",
    "                    start_index = start_index,\n",
    "                    end_index = end_index,\n",
    "                    start_logit = start_logits[start_index],\n",
    "                    end_logit = end_logits[end_index]\n",
    "                )\n",
    "            )\n",
    "    # sort prelim_preds in descending score order\n",
    "    prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "    return prelim_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60105d-4d88-4934-851e-c0996e51c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_predictions(prelim_preds, nbest, tokenizer):\n",
    "    # keep track of all best predictions\n",
    "\n",
    "    # This will be the pool from which answer probabilities are computed \n",
    "    BestPrediction = collections.namedtuple(\n",
    "        \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "    nbest_predictions = []\n",
    "    seen_predictions = []\n",
    "    for pred in prelim_preds:\n",
    "        if len(nbest_predictions) >= nbest: \n",
    "            break\n",
    "        if pred.start_index > 0: # non-null answers have start_index > 0\n",
    "\n",
    "            toks = tokens[pred.start_index : pred.end_index+1]\n",
    "            text = get_clean_text(toks, tokenizer)\n",
    "\n",
    "            # if this text has been seen already - skip it\n",
    "            if text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            # flag text as being seen\n",
    "            seen_predictions.append(text) \n",
    "\n",
    "            # add this text to a pruned list of the top nbest predictions\n",
    "            nbest_predictions.append(\n",
    "                BestPrediction(\n",
    "                    text=text, \n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    # Add the null prediction\n",
    "    nbest_predictions.append(\n",
    "        BestPrediction(\n",
    "            text=\"\", \n",
    "            start_logit=start_logits[0], \n",
    "            end_logit=end_logits[0]\n",
    "            )\n",
    "        )\n",
    "    return nbest_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd53c51-3db8-4966-9e63-b3a5d7f72f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_difference(predictions):\n",
    "    \"\"\" Assumes that the null answer is always the last prediction \"\"\"\n",
    "    score_null = predictions[-1].start_logit + predictions[-1].end_logit\n",
    "    score_non_null = predictions[0].start_logit + predictions[0].end_logit\n",
    "    return score_null - score_non_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33b2b8-535a-4024-bdc7-5e03c0582c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_prediction(example, tokenizer, nbest=10, null_threshold=1.0):\n",
    "    \n",
    "    inputs = get_qa_inputs(example, tokenizer)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    # get sensible preliminary predictions, sorted by score\n",
    "    prelim_preds = preliminary_predictions(output.start_logits, \n",
    "                                           output.end_logits, \n",
    "                                           inputs['input_ids'],\n",
    "                                           nbest)\n",
    "    \n",
    "    # narrow that down to the top nbest predictions\n",
    "    nbest_preds = best_predictions(prelim_preds, nbest, tokenizer)\n",
    "\n",
    "    # compute the probability of each prediction - nice but not necessary\n",
    "    probabilities = prediction_probabilities(nbest_preds)\n",
    "        \n",
    "    # compute score difference\n",
    "    score_difference = compute_score_difference(nbest_preds)\n",
    "\n",
    "    # if score difference > threshold, return the null answer\n",
    "    if score_difference > null_threshold:\n",
    "        return \"\", probabilities[-1]\n",
    "    else:\n",
    "        return nbest_preds[0].text, probabilities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45667134-38f7-44aa-8d90-a1594530ac84",
   "metadata": {},
   "source": [
    "We are no able to detect 'No Answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac90173-ca79-46e1-8346-01f27edc6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example.question_text)\n",
    "get_robust_prediction(example, tokenizer, nbest=10, null_threshold=best_f1_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08599f8-d8f5-4f8e-8a85-22fa23c78cf6",
   "metadata": {},
   "source": [
    "# Using A/B Testing to make model deployment decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ca07e-5dac-457f-99ad-b0b6b6945a0f",
   "metadata": {},
   "source": [
    "Now we have all the tools to make decisions on whether a model is better than the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97347618-32a8-40fd-8d96-bed0635a687d",
   "metadata": {},
   "source": [
    "### Home assistant device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d670a-bdfb-428d-a848-22dcab358d37",
   "metadata": {},
   "source": [
    "We have been tasked to evaluate whether a new model recently trained is better than our current model in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b63f5d-d16f-476b-8345-1085eaeb2eb1",
   "metadata": {},
   "source": [
    "While talking to our business we are able to enumerate our priorities in order of importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4e831-3241-4a64-82bb-00e155a92032",
   "metadata": {},
   "source": [
    "1) Our users should receive their answer quickly, and the threshold for the isolated prediction of the model is 0.01 seconds\n",
    "2) Its important to not surface answers when questions are either malformed or don't have enough context to be answered\n",
    "3) Accuracy is important to the level where uses care about factuality without too much concern with answers having exact words as their ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088e34f-ada7-4b72-b77a-f9bfbaf6361a",
   "metadata": {},
   "source": [
    "We are looking for the f1 score of \"has answer\" and \"no answer\" separately, so we can weight them according to the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42ef49-1b68-421c-b12f-c34f10741f50",
   "metadata": {},
   "source": [
    "Also, we want to measure latency as: time to predict 10000 questions from our user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e79e9-d203-4610-abb5-4f675d89c070",
   "metadata": {},
   "source": [
    "Given this scenario we write the following objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a0161-83c3-4967-9154-6f8bcb75d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_assistant_score(has_answer_f1, no_answer_f1, prediction_time):\n",
    "    return ((0.2 * has_answer_f1 + 0.3 * no_answer_f1) - (0.5 * (10000 * prediction_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d49cd1-6df1-4e7f-bad8-d5ec63582f14",
   "metadata": {},
   "source": [
    "We assign very high weight (0.5) to our prediction time, while dividing the remaining between our both F1 scores, giving slightly higher importance to \"No Answer\" scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa72b60a-8579-46c6-9ed0-4aa643ce2fb5",
   "metadata": {},
   "source": [
    "Lets evaluate now our BERT model given the above Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c19d55-5f5d-4fb2-896e-a2ff4553b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(BERT_MODEL_HF_PATH, dataset, BERT_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addafb81-7cbe-4abf-b5fd-e7523f41a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c106bb-2417-47bd-a0fc-005978d86474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model {BERT_MODEL_TYPE} score is {home_assistant_score(result['HasAns_f1'], result['NoAns_f1'], result['prediction_time'])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343c746-88a7-4582-b7f3-63d07bc24106",
   "metadata": {},
   "source": [
    "Ok, now we have a number that quantifies our business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d3cd6-5450-431f-b151-8bc665f5bfe2",
   "metadata": {},
   "source": [
    "One of our data scientists, have found a 'faster' version of BERT, called 'distilled bert' that has **40%** less parameters and is **60%** faster while preserving 95% of BERT's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408100c8-feaf-40b1-8459-80b1b78c00f5",
   "metadata": {},
   "source": [
    "Lets now evaluate this model using our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07d8c0-3d20-4fc7-8ab3-0623dd12a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_distilled = evaluate(model_name_or_path=DISTILBERT_MODEL_HF_PATH, dataset=dataset, output_dir=DISTILBERT_OUTPUT_DIR, model_type=DISTILBERT_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e388e98-634f-4c4d-ab92-e3354fb534d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7444125-e3db-4bbc-807f-a06df29839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model {DISTILBERT_MODEL_TYPE} score is {home_assistant_score(result_distilled['HasAns_f1'], result_distilled['NoAns_f1'], result_distilled['prediction_time'])}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218d167-4998-4e99-be09-acce17754889",
   "metadata": {},
   "source": [
    "The Distilled Version of BERT achieves exactly what it was described, it produces predictions in almost half of the speed of the traditional BERT with very minimal impacts to performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3dbc-2901-4ad6-b66f-03a32187f06c",
   "metadata": {},
   "source": [
    "The recommendation in the above case is to switch **BERT** model for the **DistilledBERT** as it improves business metrics by **3x**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd3b16-b0df-4bad-bd2d-dede15322c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
